# Curriculum
## **Week 1 - Basis**
- **Topic** 
  - Activation Functions (ReLU, Sigmoid)
  - Loss Functions (Cross-Entropy, Binary Cross-Entropy)
  - Linear Layer
- **Week 1 Exercise**
  - Compare NumPy implementation with Torch functions

## **Week 2 - CNN**
- **Topic**
  - Convolution Layer (2D)
  - MaxPooling (2D)
  - Flatten Layer
- **Week 2 Exercise**
  - Compare NumPy/Torch model accuracy with Linear/CNN models

## **Week 3 - Optimizer/Embedding**
- **Topic**
  - Dropout
  - Embedding
  - Positional Encoding
  - Optimizers (SGD, momentum, Adam)
- **Week 3 Exercise**
  - Compare NumPy model accuracy with different Optimizers

## **Week 4 - RNN/Tokenizer**
- **Topic**
  - RNN
  - Word Tokenzizer
- **Week 4 Exercise**
  - Train an RNN model using a word tokenizer and monitor only the loss decline.

## **Week 5 - Normalization/Residual Block**
- **Topic**
  - LayerNorm
  - BatchNorm
- **Week 5 Exercise**
  - compare w/wo Normalization Layers in terms of converge speed/accuracy

## **Week 6 - Attention**
- **Topic**
  - Attention
- **Week 6 Exercise**
  - Visualize Attention layer
  - Visualize Masked Attention map

</br>
</br>
</br>
</br>

## there are 6 week curriculum, and each week folder has description markdown file

<details>
  <summary> description file example </summary>

  ![exercise_example](../images/exercise_example.png)
</details>
