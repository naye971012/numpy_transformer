## **Week 1 - Basis**
- **Topic** 
  - Activation Functions (ReLU, Sigmoid)
  - Loss Functions (Cross-Entropy, Binary Cross-Entropy)
  - Linear Layer
- **Week 1 Exercise**
  - Compare NumPy implementation with Torch functions

</br>

## Exercise Description
- fill Relu, Sigmoid, BCEloss, Linear functions

</br>

## Excepted output

![week_1_output](images/week_1_output.PNG)