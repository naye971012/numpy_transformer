import numpy as np
from typing import Tuple, Optional

#https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5
#https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net/src/layers/convolutional.py

class Conv2d_np:
    """
    conv2d layer with numpy
    """
    def __init__(
        self, 
        in_c: int, 
        out_c: int, 
        kernel_shape: tuple = (3,3),
        padding: str = 'same', #valid or same
        stride: int = 1
    ):
        """
        :param w -  4D tensor with shape (h_f, w_f, c_f, n_f)
        :param b - 1D tensor with shape (n_f, )
        :param padding - flag describing type of activation padding valid/same
        :param stride - stride along width and height of input volume
        ------------------------------------------------------------------------
        h_f - height of filter volume
        w_f - width of filter volume
        c_f - number of channels of filter volume
        n_f - number of filters in filter volume
        """
        self.params = dict()
        self.grads = dict()
        
        self.in_c = in_c
        self.out_c = out_c
        self.k_w , self.k_h = kernel_shape
        
        limit = np.sqrt(2 / float(in_c))
        W = np.random.normal(0.0, limit, size=( self.k_h, self.k_w, self.in_c, self.out_c))
        b = np.zeros(shape = ( self.out_c , ) )
        
        self.params['W'], self.params['b'] = W, b
        self._padding = padding
        self._stride = stride
        self.grads['dW'], self.grads['db'] = None, None
        self._a_prev = None
        
    def forward(self, a_prev: np.array) -> np.array:

        a_prev = np.transpose(a_prev , axes=(0,2,3,1) )
        # n c h_in w_in -> n h_in w_in c
        """
        :param a_prev - 4D tensor with shape (n, h_in, w_in, c)
        :output 4D tensor with shape (n, h_out, w_out, n_f)
        ------------------------------------------------------------------------
        n - number of examples in batch
        w_in - width of input volume
        h_in - width of input volume
        w_out - width of input volume
        h_out - width of input volume
        c - number of channels of the input volume
        n_f - number of filters in filter volume
        """
        
        self._a_prev = np.array(a_prev, copy=True)
        output_shape = self.calculate_output_dims(input_dims=a_prev.shape)
        n, h_in, w_in, _ = a_prev.shape
        _, h_out, w_out, _ = output_shape
        h_f, w_f, _, n_f = self.params['W'].shape
        pad = self.calculate_pad_dims()
        a_prev_pad = self.pad(array=a_prev, pad=pad)
        output = np.zeros(output_shape)

        for i in range(h_out):
            for j in range(w_out):
                h_start = i * self._stride
                h_end = h_start + h_f
                w_start = j * self._stride
                w_end = w_start + w_f

                output[:, i, j, :] = np.sum(
                    a_prev_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] *
                    self.params['W'][np.newaxis, :, :, :],
                    axis=(1, 2, 3)
                )

        temp = output + self.params['b'] # [# of batch, height, width, channel]
        temp = np.transpose(temp,axes=(0,3,1,2)) # [# of batch, # of channel, height, width]

        return temp


    def backward(self, da_curr: np.array) -> np.array:
        da_curr = np.transpose(da_curr , axes=(0,2,3,1) )
        # n c h_in w_in -> n h_in w_in c
        """
        :param da_curr - 4D tensor with shape (n, h_out, w_out, n_f)
        :output 4D tensor with shape (n, h_in, w_in, c)
        ------------------------------------------------------------------------
        n - number of examples in batch
        w_in - width of input volume
        h_in - width of input volume
        w_out - width of input volume
        h_out - width of input volume
        c - number of channels of the input volume
        n_f - number of filters in filter volume
        """
        _, h_out, w_out, _ = da_curr.shape
        n, h_in, w_in, _ = self._a_prev.shape
        h_f, w_f, _, _ = self.params['W'].shape
        pad = self.calculate_pad_dims()
        a_prev_pad = self.pad(array=self._a_prev, pad=pad)
        output = np.zeros_like(a_prev_pad)

        self.grads['db'] = da_curr.sum(axis=(0, 1, 2)) / n
        self.grads['dW'] = np.zeros_like(self.params['W'])

        for i in range(h_out):
            for j in range(w_out):
                h_start = i * self._stride
                h_end = h_start + h_f
                w_start = j * self._stride
                w_end = w_start + w_f
                output[:, h_start:h_end, w_start:w_end, :] += np.sum(
                    self.params['W'][np.newaxis, :, :, :, :] *
                    da_curr[:, i:i+1, j:j+1, np.newaxis, :],
                    axis=4
                )
                self.grads['dW'] += np.sum(
                    a_prev_pad[:, h_start:h_end, w_start:w_end, :, np.newaxis] *
                    da_curr[:, i:i+1, j:j+1, np.newaxis, :],
                    axis=0
                )

        self.grads['dW'] /= n
        temp = output[:, pad[0]:pad[0]+h_in, pad[1]:pad[1]+w_in, :] # [# of batch, height, width, channel]
        temp = np.transpose(temp,axes=(0,3,1,2)) # [# of batch, # of channel, height, width]
        
        return temp
        
    def calculate_output_dims(
        self, input_dims: Tuple[int, int, int, int]
    ) -> Tuple[int, int, int, int]:
        """
        :param input_dims - 4 element tuple (n, h_in, w_in, c)
        :output 4 element tuple (n, h_out, w_out, n_f)
        ------------------------------------------------------------------------
        n - number of examples in batch
        w_in - width of input volume
        h_in - width of input volume
        w_out - width of input volume
        h_out - width of input volume
        c - number of channels of the input volume
        n_f - number of filters in filter volume
        """
        n, h_in, w_in, _ = input_dims
        h_f, w_f, _, n_f = self.params['W'].shape
        if self._padding == 'same':
            return n, h_in, w_in, n_f
        elif self._padding == 'valid':
            h_out = (h_in - h_f) // self._stride + 1
            w_out = (w_in - w_f) // self._stride + 1
            return n, h_out, w_out, n_f
        else:
            return None

    def calculate_pad_dims(self) -> Tuple[int, int]:
        """
        :output - 2 element tuple (h_pad, w_pad)
        ------------------------------------------------------------------------
        h_pad - single side padding on height of the volume
        w_pad - single side padding on width of the volume
        """
        if self._padding == 'same':
            h_f, w_f, _, _ = self.params['W'].shape
            return (h_f - 1) // 2, (w_f - 1) // 2
        elif self._padding == 'valid':
            return 0, 0
        else:
            return None

    def pad(self, array: np.array, pad: Tuple[int, int]) -> np.array:
        """
        :param array -  4D tensor with shape (n, h_in, w_in, c)
        :param pad - 2 element tuple (h_pad, w_pad)
        :output 4D tensor with shape (n, h_out, w_out, n_f)
        ------------------------------------------------------------------------
        n - number of examples in batch
        w_in - width of input volume
        h_in - width of input volume
        w_out - width of input volume
        h_out - width of input volume
        c - number of channels of the input volume
        n_f - number of filters in filter volume
        h_pad - single side padding on height of the volume
        w_pad - single side padding on width of the volume
        """
        return np.pad(
            array=array,
            pad_width=((0, 0), (pad[0], pad[0]), (pad[1], pad[1]), (0, 0)),
            mode='constant'
        )
        
    def __call__(self, x):
        return self.forward(x)


#for check dim
if __name__ == "__main__":
    layer = Conv2d_np(1,3,(3,3))
    
    in_p = np.random.randn(1,1,28,28)
    print(in_p.shape)
    out_p = layer(in_p)
    print(out_p.shape)
    back_out_p = layer.backward(out_p)
    print(back_out_p.shape)
    
    