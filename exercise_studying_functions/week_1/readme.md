## **Week 1 - Basis**
- **Topic** 
  - Activation Functions (ReLU, Sigmoid)
  - Loss Functions (Cross-Entropy, Binary Cross-Entropy)
  - Linear Layer
- **Week 1 Exercise**
  - Compare NumPy implementation with Torch functions

</br>

## Exercise Description
- fill Relu, Sigmoid, BCEloss, Linear functions

</br>

## Excepted output

![week_1_output](https://github.com/naye971012/numpy_transformer/assets/74105909/76385e9a-f546-4ccb-92ba-37d2530d3e62)
